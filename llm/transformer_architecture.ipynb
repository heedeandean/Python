{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ⭐️ Transformer Architecture\n",
        "* ⭐️ encoder: 언어 이해\n",
        "* ⭐️ decoder: 언어 생성\n",
        "* ⭐️ 병렬 연산 O: 모든 입력을 동시에 처리\n",
        "* 대부분 LLM(Large Language Model, 대규모 언어 모델)에서 사용: 딥러닝 기반\n",
        "* 구글 논문: Attention is All you need.\n",
        "* self-attention: 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산.\n",
        "---\n",
        "### cf. RRN(Recurrent Neural Network, 순환신경망)\n",
        "* 입력을 순차적⭐️으로 처리. 즉 병렬 처리 X\n",
        "---"
      ],
      "metadata": {
        "id": "8-EHdGxoUx7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1️⃣ 토큰화(tokenization): 텍스트를 적절한 단위로 잘라 숫자 ID를 부여.\n",
        "* 사전(vocabulary): 토큰과 매칭된 숫자 ID 기록.\n",
        "* ex) 한글: (자음과 모음 < 음절 < 단어) => 작은 단위 < 큰단위\n",
        "  * 큰 단위 기준으로 자른다면\n",
        "    * (-) OOV(Out Of Vocabulary, 사전에 없는 단어): 사전에 없으면 처리하지 못하는 문제.\n",
        "    * (-) 텍스트에 등장하는 단어 수만큼 토큰 ID가 필요하기 때문에 사전의 크기가 커짐.\n",
        "    * (+) 텍스트의 의미가 잘 유지됨.\n",
        "\n",
        "* ⭐️ subword(多): 등장 빈도에 따라 토큰화 단위 결정.\n",
        "  * 자주 나오는 단어는 그대로 의미 유지, 드물게 나오는 단어는 작게 나눠 사전 크기가 커지지 않도록 함.\n",
        "  * 한글의 경우 보통 음절(ex. 특수문자, 이모티콘)과 단어(ex. 유명인 이름)로 토큰화."
      ],
      "metadata": {
        "id": "C-T5cLukYcRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 띄어쓰기 단위로 분리 (실습 편의상, 실무에서는 subword 주로 사용.)\n",
        "input_text = '나는 최근 미국 여행을 다녀왔다'\n",
        "input_text_list = input_text.split()\n",
        "print(input_text_list)"
      ],
      "metadata": {
        "id": "em8jvF-wXhRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ace4a3b-a08f-4ce5-d0d0-046ec5cb6ae8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나는', '최근', '미국', '여행을', '다녀왔다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰 딕셔너리\n",
        "str2idx = {word: idx for idx, word in enumerate(input_text_list)}\n",
        "print(str2idx)"
      ],
      "metadata": {
        "id": "DzeVWB-VcCEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b59d7768-ed0b-4969-e9c8-a73e55f0a89e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'나는': 0, '최근': 1, '미국': 2, '여행을': 3, '다녀왔다': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx2str = {idx: word for idx, word in enumerate(input_text_list)}\n",
        "print(idx2str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkHeUMiuAdXM",
        "outputId": "866b1ecd-8ff5-4daf-dc8a-c4d3a78fbbdd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '나는', 1: '최근', 2: '미국', 3: '여행을', 4: '다녀왔다'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = [str2idx[word] for word in input_text_list]\n",
        "print(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go9h3YUqAuny",
        "outputId": "7862ed3b-8390-4a7a-a0e0-5cb35c62f068"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2️⃣ 토큰 embedding: 데이터 -> vector(숫자 집합) 변환 (의미가 담김)\n"
      ],
      "metadata": {
        "id": "_YglzsZIDVQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 토큰 ID 1개 => 16차원의 벡터로 변환.\n",
        "embedding_dim = 16\n",
        "embed_layer = nn.Embedding(len(str2idx), embedding_dim) # 임베딩 층: 임의의 숫자 집합으로 변환(의미 X), 실무에선 이 층도 학습시켜서 의미를 담음.\n",
        "print(embed_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJmz84Y1A61q",
        "outputId": "3d7525af-633e-41a9-9ade-29777e29c02f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(5, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = embed_layer(torch.tensor(input_ids))\n",
        "print(token_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLEX3EPCE8jQ",
        "outputId": "ce546764-4acd-47f6-e807-b8fecb284c89"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1160, -0.7262, -1.3192,  0.8191,  0.7993, -0.7065,  0.4225,  0.2210,\n",
            "          1.2648, -0.0686, -1.2311,  0.4734,  0.9042,  0.3324,  1.0393, -0.8071],\n",
            "        [ 0.9720,  0.5177, -0.3043, -2.1011, -0.0873,  0.5373,  0.2749,  1.8464,\n",
            "         -0.9620,  0.1327, -1.5265,  0.8904,  0.0932, -2.2419, -0.9434, -0.4522],\n",
            "        [ 0.1612, -0.8149, -1.4896, -0.9075,  1.1382, -0.1885,  1.0224,  0.4806,\n",
            "         -0.3866, -0.6041,  0.3711,  2.1835, -0.0756,  2.0240,  0.3116, -0.7648],\n",
            "        [ 1.7334,  1.7280,  0.6350,  0.1689, -1.3094, -3.5773,  0.3951, -0.2788,\n",
            "          1.8887, -0.1501, -1.0158, -0.5792, -0.8352, -0.1427,  0.4078, -1.5921],\n",
            "        [-1.5153, -0.5584, -1.8897, -0.3588, -1.6828, -1.3027,  0.0464,  1.0681,\n",
            "         -0.5563,  1.7615,  0.5348, -0.6170, -0.5167,  0.0480,  0.9180,  0.8601]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embeddings.unsqueeze(0)\n",
        "print(token_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQxAtsc7FOk-",
        "outputId": "a1729071-0984-4141-e9e1-a20b1397801e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1160, -0.7262, -1.3192,  0.8191,  0.7993, -0.7065,  0.4225,\n",
            "           0.2210,  1.2648, -0.0686, -1.2311,  0.4734,  0.9042,  0.3324,\n",
            "           1.0393, -0.8071],\n",
            "         [ 0.9720,  0.5177, -0.3043, -2.1011, -0.0873,  0.5373,  0.2749,\n",
            "           1.8464, -0.9620,  0.1327, -1.5265,  0.8904,  0.0932, -2.2419,\n",
            "          -0.9434, -0.4522],\n",
            "         [ 0.1612, -0.8149, -1.4896, -0.9075,  1.1382, -0.1885,  1.0224,\n",
            "           0.4806, -0.3866, -0.6041,  0.3711,  2.1835, -0.0756,  2.0240,\n",
            "           0.3116, -0.7648],\n",
            "         [ 1.7334,  1.7280,  0.6350,  0.1689, -1.3094, -3.5773,  0.3951,\n",
            "          -0.2788,  1.8887, -0.1501, -1.0158, -0.5792, -0.8352, -0.1427,\n",
            "           0.4078, -1.5921],\n",
            "         [-1.5153, -0.5584, -1.8897, -0.3588, -1.6828, -1.3027,  0.0464,\n",
            "           1.0681, -0.5563,  1.7615,  0.5348, -0.6170, -0.5167,  0.0480,\n",
            "           0.9180,  0.8601]]], grad_fn=<UnsqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings.shape # 문장 1개, 토큰 5개(각 임베딩 16차원)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAs_vpA9F5RW",
        "outputId": "5c2d0ab3-b098-41cb-8727-b77923bee03e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3️⃣ 위치 인코딩(position encoding)\n",
        "* 텍스트에서 순서는 매우 중요한 정보"
      ],
      "metadata": {
        "id": "2PoYq4LCL02V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 절대적(absolute) 위치 인코딩: 토큰의 위치에 따라 고정된 임베딩을 더함. (<-> 상대적(relative) 위치 인코딩)\n",
        "max_position = 12\n",
        "position_embed_layer = nn.Embedding(max_position, embedding_dim) # 위치 인코딩 층\n",
        "print(position_embed_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu5om3jcL6E6",
        "outputId": "1789cee8-2764-45c6-d2a2-9d76a839f0b8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(12, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0)\n",
        "print(position_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObVvzAi8Mq47",
        "outputId": "cab9c9f6-35d8-44d7-e4f9-ec7b68ce6cab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1, 2, 3, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position_encodings = position_embed_layer(position_ids)\n",
        "print(position_encodings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zddnyewDNsFr",
        "outputId": "d75adc9d-8ca7-40ed-a6e9-30259d626f3f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.2843, -0.9143,  0.7962, -0.8312, -0.7476,  0.3424,  1.6838,\n",
            "          -0.3676, -0.9341, -0.9542,  1.0027,  1.6564, -0.5288, -0.8302,\n",
            "          -2.2409, -2.7816],\n",
            "         [-0.8405,  0.0154, -0.2683,  0.5512, -0.6088,  0.5670, -0.9249,\n",
            "           1.4955, -0.5395, -0.9027,  2.0104,  0.5816, -0.8601, -1.2781,\n",
            "          -0.7299,  1.5986],\n",
            "         [ 0.3015,  1.2039,  1.6438, -1.2443,  0.3622, -0.5476,  0.2646,\n",
            "          -0.4422, -0.8375, -1.1003, -0.2834, -1.3641, -0.5535, -0.7492,\n",
            "           0.3403,  1.3708],\n",
            "         [-1.8227, -0.5611,  1.7791, -0.7227, -1.1062,  1.9822,  0.6457,\n",
            "          -1.1013,  0.2705, -0.3542, -1.3914, -1.3609, -1.7423, -0.3242,\n",
            "          -0.4138,  0.6098],\n",
            "         [-0.7928, -0.1854,  0.1147, -1.7375, -1.7869, -0.3057, -0.4513,\n",
            "           0.8481,  0.5399, -1.6301, -1.5184,  0.1195, -0.3041,  0.0882,\n",
            "           0.4547, -0.2332]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + position_encodings\n",
        "print(input_embeddings) # 모델에 입력할 최종 임베딩"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdS-Wu8SORnT",
        "outputId": "a532753f-d55e-46d6-94b8-effbacf4dbf8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.1682, -1.6404, -0.5230, -0.0121,  0.0516, -0.3641,  2.1064,\n",
            "          -0.1466,  0.3308, -1.0228, -0.2283,  2.1298,  0.3755, -0.4978,\n",
            "          -1.2016, -3.5887],\n",
            "         [ 0.1315,  0.5331, -0.5726, -1.5499, -0.6960,  1.1043, -0.6500,\n",
            "           3.3419, -1.5016, -0.7700,  0.4838,  1.4720, -0.7668, -3.5200,\n",
            "          -1.6733,  1.1464],\n",
            "         [ 0.4627,  0.3890,  0.1542, -2.1518,  1.5004, -0.7361,  1.2870,\n",
            "           0.0385, -1.2241, -1.7044,  0.0877,  0.8194, -0.6290,  1.2748,\n",
            "           0.6519,  0.6060],\n",
            "         [-0.0893,  1.1668,  2.4141, -0.5538, -2.4156, -1.5950,  1.0408,\n",
            "          -1.3801,  2.1592, -0.5043, -2.4071, -1.9401, -2.5776, -0.4670,\n",
            "          -0.0060, -0.9824],\n",
            "         [-2.3080, -0.7438, -1.7750, -2.0962, -3.4697, -1.6084, -0.4049,\n",
            "           1.9162, -0.0164,  0.1314, -0.9836, -0.4975, -0.8209,  0.1361,\n",
            "           1.3727,  0.6269]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9SDIH0aOlha",
        "outputId": "cfd31890-e003-46f0-f74c-ca5357c7f66d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4️⃣ Attention: 사람이 단어 사이의 관계를 고민하는 과정을 딥러닝 모델이 수행할 수 있도록 모방.\n",
        "* 사람: 맥락을 반영해 단어를 재해석 => ⭐️ 가중치를 사용해 토큰 간 관계를 계산해서 관련이 깊은 단어와 그렇지 않은 단어를 구분\n",
        "1. query: 검색어\n",
        "2. key: 문서가 가진 특징 (ex. 제목, 저자 이름, 문장 속의 각 단어)\n",
        "3. value: 원하는 값"
      ],
      "metadata": {
        "id": "XIieXh3miPJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head_dim = 16\n",
        "\n",
        "weight_q = nn.Linear(embedding_dim, head_dim) # 선형 층\n",
        "weight_k = nn.Linear(embedding_dim, head_dim)\n",
        "weight_v = nn.Linear(embedding_dim, head_dim)\n",
        "\n",
        "querys = weight_q(input_embeddings)\n",
        "keys = weight_k(input_embeddings)\n",
        "values = weight_v(input_embeddings)\n",
        "\n",
        "print(querys) # (1, 5, 16)\n",
        "print(keys)\n",
        "print(values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzKzHYOiPc65",
        "outputId": "ea52b4d3-bab4-4809-c0e5-f5507952b24a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.3045e+00, -3.9089e-01,  1.3074e+00,  1.9847e-01,  2.6002e-01,\n",
            "           4.9307e-01,  1.1281e-01, -4.0106e-01,  4.4259e-01,  7.1924e-01,\n",
            "          -6.9390e-01,  7.2495e-01,  4.0594e-01,  5.3339e-01, -1.3246e+00,\n",
            "          -1.9372e-01],\n",
            "         [ 2.1203e-01, -2.4054e+00, -1.0225e+00,  1.0628e+00,  1.9424e-01,\n",
            "          -3.9157e-01, -3.9018e-01, -2.3384e-01, -1.1760e+00, -6.2021e-01,\n",
            "           6.7474e-02,  1.3634e+00, -5.4305e-01,  1.2736e+00, -1.8374e+00,\n",
            "           4.8560e-01],\n",
            "         [-4.6478e-01, -3.0420e-01, -4.2859e-01, -3.1556e-01,  3.0028e-01,\n",
            "          -3.9897e-01,  1.0369e+00,  3.7221e-01,  1.4550e-01,  1.0903e+00,\n",
            "          -3.0956e-02,  3.0964e-01, -3.2314e-01,  2.7083e-01,  2.1801e-01,\n",
            "           3.0985e-01],\n",
            "         [ 1.8702e-01,  1.2337e+00,  2.7763e-01, -1.3426e+00,  4.6012e-01,\n",
            "           1.2876e+00,  9.3376e-01, -2.5120e-01,  1.0356e+00,  1.5255e-04,\n",
            "           3.5870e-01, -1.3668e+00,  1.2642e-01,  7.1631e-01,  1.1488e+00,\n",
            "           1.1959e-01],\n",
            "         [-1.4412e+00, -2.0832e-01, -8.9188e-01, -1.1783e+00, -1.3790e-01,\n",
            "           3.4356e-01,  2.1186e+00,  8.2202e-01, -1.0780e+00,  5.2445e-01,\n",
            "          -1.0544e+00,  4.6642e-01,  1.2456e-01,  3.4394e-01, -1.1968e+00,\n",
            "           8.2943e-01]]], grad_fn=<ViewBackward0>)\n",
            "tensor([[[-1.8690e-01,  3.1069e-01, -1.3073e+00, -3.3148e-02,  6.4313e-01,\n",
            "           7.9158e-01, -4.0919e-01,  9.8973e-01,  9.6446e-01,  3.0059e-01,\n",
            "           8.2528e-01,  1.5431e+00,  1.6126e-01,  7.6689e-01,  4.4931e-01,\n",
            "           8.9941e-03],\n",
            "         [-3.1028e-01,  8.4171e-01, -3.7865e-01, -2.0259e-01,  1.2639e-01,\n",
            "          -1.2017e+00, -6.6564e-01, -1.5324e+00, -4.0558e-01, -1.0115e+00,\n",
            "          -4.9341e-01,  4.9103e-01,  1.9592e-01,  3.3573e-01, -5.1788e-02,\n",
            "          -1.1086e+00],\n",
            "         [ 2.5935e-01, -1.5322e+00, -5.7696e-01, -2.7202e-01, -2.5752e-01,\n",
            "          -6.2779e-01, -1.3551e+00,  1.0661e-01,  1.0398e+00, -9.8836e-01,\n",
            "           5.0432e-01,  1.2919e-01,  1.6155e-01,  4.2267e-01,  3.8796e-01,\n",
            "          -5.2819e-01],\n",
            "         [ 1.5693e-01,  1.4739e-01,  1.8135e+00,  6.0756e-01,  2.3814e+00,\n",
            "           4.9636e-01,  5.5307e-01,  5.5432e-06, -1.4176e+00,  8.1704e-01,\n",
            "           6.0805e-01,  5.8334e-01,  1.1614e+00, -2.3149e-01, -5.5028e-01,\n",
            "           6.2637e-01],\n",
            "         [-1.5576e+00,  4.8948e-01,  6.3691e-01,  8.1412e-01,  1.5827e+00,\n",
            "           3.0455e-01,  6.9932e-01, -1.9504e+00, -9.7775e-01, -9.7484e-01,\n",
            "           9.8445e-01,  4.9285e-01, -6.4914e-01, -3.3298e-01,  6.2581e-01,\n",
            "          -1.3692e+00]]], grad_fn=<ViewBackward0>)\n",
            "tensor([[[ 0.9499, -0.1477,  0.5875,  0.5261, -1.2235, -0.0209,  0.6410,\n",
            "          -0.6459, -0.0841, -1.6737, -0.0413, -0.3771,  0.0741,  0.8142,\n",
            "          -1.7680,  0.3695],\n",
            "         [-0.2663,  1.0814,  0.6663,  1.7162, -0.8704, -1.1307, -1.4534,\n",
            "           0.1263,  0.1043, -0.3563,  0.5969,  1.8666,  0.9232, -0.5088,\n",
            "           0.1436, -0.1928],\n",
            "         [ 0.2715,  0.4832,  0.3478,  0.0818,  0.0760, -0.5986,  0.4452,\n",
            "           0.1266,  0.3036, -0.7855,  0.0944,  0.4606, -0.3459,  1.2167,\n",
            "          -1.3081, -0.5813],\n",
            "         [-0.3947, -1.0538,  0.9695,  0.7380, -0.8447, -0.2888,  0.8205,\n",
            "           1.1019, -0.5982, -1.0213,  0.9268,  0.0608, -0.8066,  0.3296,\n",
            "          -0.2647, -0.0119],\n",
            "         [-0.7769,  0.1598,  0.1615,  0.5851,  0.7563, -0.9773,  0.5810,\n",
            "           1.6158, -1.1102,  0.5919,  0.2794,  1.8290, -0.3111,  0.4819,\n",
            "           0.6974, -1.0136]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 스케일 점곱 어텐션: 1개의 어텐션 연산 수행.\n",
        "from math import sqrt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_attention(querys, keys, values, is_causal=False):\n",
        "    dim_k = querys.size(-1) # 16\n",
        "    scores = querys @ keys.transpose(-2, -1) / sqrt(dim_k)\n",
        "    if is_causal: # 디코더 - 마스크 어텐션: 작성해야 하는 텍스트를 미리 확인하게 되는 문제 해결. 즉, 미래 시점의 토큰을 제거하여 생성된 토큰까지만 확인할 수 있도록 마스크를 추가.\n",
        "        query_length = querys.size(-2)\n",
        "        key_length = keys.size(-2)\n",
        "        temp_mask = torch.ones(query_length, key_length, dtype=torch.bool).tril(diagonal=0) # 모두 1인 행렬에 대각선 아래 부분만 1로 유지되고, 나머지는 음의 무한대로 변경해 마스크를 생성.\n",
        "        scores = scores.masked_fill(temp_mask == False, float('-inf')) # 행렬의 대각선 아래 부분만 어텐션 스코어가 남고, 위쪽은 음의 무한대가 된다.\n",
        "    weights = F.softmax(scores, dim=-1) # 총 합이 1 / 음의 무한대인 대각선 윗부분은 가중치가 0이 됨.\n",
        "    return weights @ values\n",
        "\n",
        "after_attention_embeddings = compute_attention(querys, keys, values) # 입력, 출력 형태 동일\n",
        "print(after_attention_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M5RML23imnV",
        "outputId": "ed837c2c-ae00-4ccd-b457-9ec0c66eaa8b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, token_embed_dim, head_dim, is_causal=False):\n",
        "        super().__init__()\n",
        "        self.is_causal = is_causal\n",
        "        self.weight_q = nn.Linear(token_embed_dim, head_dim)\n",
        "        self.weight_k = nn.Linear(token_embed_dim, head_dim)\n",
        "        self.weight_v = nn.Linear(token_embed_dim, head_dim)\n",
        "\n",
        "    def forward(self, querys, keys, values):\n",
        "        outputs = compute_attention(\n",
        "            self.weight_q(querys),\n",
        "            self.weight_k(keys),\n",
        "            self.weight_v(values),\n",
        "            is_causal=self.is_causal\n",
        "        )\n",
        "        return outputs\n",
        "\n",
        "attention_head = AttentionHead(embedding_dim, embedding_dim)\n",
        "after_attention_embeddings = attention_head(input_embeddings, input_embeddings, input_embeddings)\n",
        "print(after_attention_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgQrjLbn2CQl",
        "outputId": "e3ecf39e-3679-4658-8a1a-5591db613425"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 멀티 헤드 어텐션: 여러 어텐션 연산을 헤드의 수(n_head)만큼 동시에 수행, 단어 사이의 관계를 계산.\n",
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, token_embed_dim, d_model, n_head, is_causal=False):\n",
        "        super().__init__()\n",
        "        self.n_head = n_head\n",
        "        self.is_causal = is_causal\n",
        "        self.weight_q = nn.Linear(token_embed_dim, d_model)\n",
        "        self.weight_k = nn.Linear(token_embed_dim, d_model)\n",
        "        self.weight_v = nn.Linear(token_embed_dim, d_model)\n",
        "        self.concat_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, querys, keys, values):\n",
        "        B, T, C = querys.size()\n",
        "\n",
        "        querys = self.weight_q(querys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        keys = self.weight_k(keys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        values = self.weight_v(values).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        attention = compute_attention(querys, keys, values, is_causal=self.is_causal) # n_head번의 스케일 점곱 어텐션\n",
        "        output = attention.transpose(1, 2).contiguous().view(B, T, C) # 어텐션 결과를 연결\n",
        "        output = self.concat_linear(output) # 마지막 선형 층\n",
        "        return output\n",
        "\n",
        "n_head = 4\n",
        "mh_attention = MultiheadAttention(embedding_dim, embedding_dim, n_head)\n",
        "after_attention_embeddings = mh_attention(input_embeddings, input_embeddings, input_embeddings)\n",
        "print(after_attention_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee28be61-5055-497b-ecea-a30d0f6f58c4",
        "id": "8PynPxbR6YrA"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5️⃣ 정규화(normalization): 딥러닝 모델의 모든 입력 변수가 비슷한 범위와 분포를 갖도록 조정하면 각 입력 변수의 중요성을 공정하게 반영하여 더 정확한 예측을 할 수 있음.\n",
        "* 1. 층(layer) 정규화\n",
        "    - 각 층의 입력 데이터 분포를 균일하게 만들어 딥러닝 모델이 원할하게 학습되도록 함.\n",
        "    - 자연어 처리\n",
        "    - Transformer Architecture\n",
        "    - 층과 층 사이에 정규화\n",
        "    - 각 토큰 임베딩의 평균과 표준편차를 구함.\n",
        "    - 사전 정규화(pre-norm)多: 먼저 층 정규화를 적용하고 어텐션과 피드 포워드 층을 통과했을 때 학습이 더 안정적. (cf. 사후 정규화(post-norm))\n",
        "\n",
        "* 2. batch 정규화: 이미지 처리"
      ],
      "metadata": {
        "id": "Q22aLSWZ7dvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 층 정규화, 사전 정규화\n",
        "norm = nn.LayerNorm(embedding_dim) # 층 정규화 레이어\n",
        "norm_x = norm(input_embeddings) # 정규화된 임베딩\n",
        "norm_x.shape"
      ],
      "metadata": {
        "id": "KsEHlku24y3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f45d5218-7a82-4477-8c03-6cdeca3484b8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "norm_x.mean(dim=-1).data, norm_x.std(dim=-1).data # 평균, 표준편차"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fBWvw49Be6R",
        "outputId": "c674cb7b-e83a-4d74-edb7-dc5f8d03627d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-1.4901e-08, -2.9802e-08, -1.4901e-08,  0.0000e+00,  2.9802e-08]]),\n",
              " tensor([[1.0328, 1.0328, 1.0328, 1.0328, 1.0328]]))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# feed forward layer, 완전 연결 층(fully connected layer)\n",
        "# > 입력 텍스트 전체를 이해.\n",
        "# > 데이터의 특징을 학습.\n",
        "class PreLayerNormFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, dim_feedforward, dropout):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward) # 선형 층\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model) # 선형 층\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = nn.GELU()\n",
        "        self.norm = nn.LayerNorm(d_model) # 층 정규화\n",
        "\n",
        "    def forward(self, src):\n",
        "        x = self.norm(src)\n",
        "        x = x + self.linear2(self.dropout1(self.activation(self.linear1(x))))\n",
        "        x = self.dropout2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "sSSvOWSRBgDq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6️⃣ 인코더\n",
        "* 자연어 이해(Natural Language Understanding, NLU)\n",
        "* 양방향"
      ],
      "metadata": {
        "id": "jtfXVAa4Z-v6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_head, dim_feedforward, dropout):\n",
        "        super().__init__()\n",
        "        self.attn = MultiheadAttention(d_model, d_model, n_head)\n",
        "        self.norm1 = nn.LayerNorm(d_model) # 층 정규화\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        norm_x = self.norm1(src)\n",
        "        attn_output = self.attn(norm_x, norm_x, norm_x)\n",
        "        x = src + self.dropout1(attn_output) # 잔차 연결(residual connection): 안정적인 학습이 가능하도록 도와줌.\n",
        "        return self.feed_forward(x)\n",
        "\n",
        "\n",
        "import copy\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)]) # 깊은 복사\n",
        "\n",
        "class TransformerEncoder(nn.Module): # 인코더 층을 N번 반복 수행.\n",
        "    def __init__(self, encoder_layer, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(self, src):\n",
        "        output = src\n",
        "        for mod in self.layers:\n",
        "            output = mod(output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "OYVMtaW-Z93E"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7️⃣ 디코더\n",
        "* 자연어 생성(Natural Language Generation, NLG)\n",
        "* 단방향\n",
        "* 사람이 글을 쓸 때 앞 단어부터 순차적으로 작성하는 것처럼 트랜스포머 모델도 앞에서 생성한 토큰을 기반으로 다음 토큰을 생성.\n",
        "> 순차적으로 생성: 인과적(causal) == 자기 회귀적(auto-regressive)"
      ],
      "metadata": {
        "id": "AwrGlhh9Z9hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiheadAttention(d_model, d_model, nhead)\n",
        "        self.multihead_attn = MultiheadAttention(d_model, d_model, nhead)\n",
        "        self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, is_causal=True):\n",
        "        x = self.norm1(tgt)\n",
        "        x = x + self.dropout1(self.self_attn(x, x, x, is_causal=is_causal))\n",
        "\n",
        "        # ⭐️cross attention: 인코더의 결과를 디코더가 활용\n",
        "        x = self.norm2(x)\n",
        "        x = x + self.dropout2(self.multihead_attn(x, encoder_output, encoder_output))\n",
        "\n",
        "        return self.feed_forward(x)\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, decoder_layer, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = get_clones(decoder_layer, num_layers) # 디코더 층을 N번 반복.\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, tgt, src):\n",
        "        output = tgt\n",
        "        for mod in self.layers:\n",
        "            output = mod(output, src)\n",
        "        return output"
      ],
      "metadata": {
        "id": "3zBMTmFKjKJO"
      },
      "execution_count": 22,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONHdlDyBK7LDEaOJqjZwVB"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}